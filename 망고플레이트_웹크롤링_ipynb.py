# -*- coding: utf-8 -*-
"""망고플레이트  웹크롤링.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vcH2pQpSaDsgmyfmJaNTopbrztAC1MQz

##참고
https://github.com/yoonkt200/python-data-analysis/blob/master/chapter4/(4%EC%87%84%20%ED%81%AC%EB%A1%A4%EB%A7%81%20%EB%B3%80%EA%B2%BD%20%EC%98%88%EC%A0%9C)%20mangoplate-crawling.ipynb

--------------------------
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install openpyxl
!pip install selenium
!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
!pip install webdriver_manager
from urllib.request import urlopen

import pandas as pd
from selenium import webdriver
from bs4 import BeautifulSoup
import re
import time
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options as ChromeOptions

# Install chromedriver_autoinstaller if not already installed
try:
    import chromedriver_autoinstaller
except ImportError:
    !pip install chromedriver_autoinstaller
    import chromedriver_autoinstaller

# Automatically install and set up ChromeDriver
chromedriver_autoinstaller.install()

chrome_options = ChromeOptions()
chrome_options.add_argument('--headless')  # Run in headless mode
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')


# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

# # Define the source URL
# # '연남카페'를 검색어로 넣었다
# source_url = "https://www.mangoplate.com/search/%EC%97%B0%EB%82%A8%20%EC%B9%B4%ED%8E%98"

# # Create a Chrome WebDriver with the defined options
# driver = webdriver.Chrome(options=chrome_options)
# driver.get(source_url)
# req = driver.page_source
# soup = BeautifulSoup(req, "html.parser")
# info_list = soup.find_all(name="div", attrs={"class":"info"})

tmp1 = 'https://search.naver.com/search.naver?where=kin'
# 네이버 지식인 Url 뒤에 kin부분만 바꾸면 뉴스 블로그 카페 다 가능하다.
html = tmp1 + '&sm=tab_jum&ie=utf8&query={key_word}&kin_start={num}'
# 네이버 지식인 페이지수 결정 Url

response = urlopen(html.format(num = 1, key_word = urllib.parse.quote('감기증상')))
#urlib.parse.quite를 사용하는 이유는 Url에 한글을 섞으면 오류가 발생하는데 그 오류를 방지하기 위한 함수이다.
#코인세탁방에 본인이 원하는 키워드 입력
soup = BeautifulSoup(response, "html.parser")

page_urls = []
page_url_base = "https://www.mangoplate.com"
for index in range(0, len(info_list)):
    info = info_list[index]
    review_url = info.find(name="a")
    if review_url is not None:
        page_urls.append(page_url_base + review_url.get("href"))

# 중복 url을 제거합니다.
page_urls = list(set(page_urls))

# 열 개의 페이지를 출력합니다.
for page in page_urls[:10]:
    print(page)

# 크롤링에 사용한 브라우저를 종료합니다.
driver.close()

# import pandas as pd
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from bs4 import BeautifulSoup

columns = ['score', 'review']
df = pd.DataFrame(columns=columns)

# driver = webdriver.Chrome(mac_path)  # for Mac
driver = webdriver.Chrome(options=chrome_options)
for page_url in page_urls:

    # 상세보기 페이지에 접속합니다
    driver.get(page_url)

    # 리뷰를 크롤링합니다
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    review_list = soup.find(name="section", attrs={"class": "RestaurantReviewList"})

    # 평가 정보('맛있다' 마크)를 가져옵니다.
    review_recommend_list = review_list.find_all(name="div", attrs={"class": "RestaurantReviewItem__Rating RestaurantReviewItem__Rating--Recommend"})
    review_recommend_list01 = review_list.find_all(name="div", attrs={"class": "RestaurantReviewItem__Rating RestaurantReviewItem__Rating--Ok"})
    # review_recommend_list02 = review_list.find_all(name="div", attrs={"class":"RestaurantReviewList__Rating RestaurantReviewList__NotRecommend"})

    # 리뷰를 가져옵니다.
    review_text_list = review_list.find_all(name="div", attrs={"class": "RestaurantReviewItem__ReviewContent"})

    for score, review_text in zip(review_recommend_list, review_text_list):
        score_text = score.find(name="span").text if score else None
        review_text_content = review_text.find(name="p").text if review_text else None

        row = [score_text, review_text_content]
        series = pd.Series(row, index=df.columns)
        df = df.append(series, ignore_index=True)

    for score, review_text in zip(review_recommend_list01, review_text_list):
        score_text = score.find(name="span").text if score else None
    review_text_content = review_text.find(name="p").text if review_text else None

    row = [score_text, review_text_content]
    series = pd.Series(row, index=df.columns)
    df = df.append(series, ignore_index=True)


driver.close()

df

excel_file_path = "cafe review 04.xlsx"  # 저장할 엑셀 파일 경로 및 이름을 정의해주세요.
df.to_excel(excel_file_path, index=False)
print("DataFrame이 엑셀 파일로 저장되었습니다:", excel_file_path)