# -*- coding: utf-8 -*-
"""망고플레이트 리뷰 감성분석(강남).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PrmqKeUl6raQi3ff6LgwesWOdFDymkz2

###1
교안대로 따라서
"""

!pip install konlpy

# 이미 앞서 다뤄본 적이 있는 기본적인 라이브러리
import numpy as np
import pandas as pd

import json
import re

from tqdm.notebook import tqdm

from konlpy.tag import Okt # komoran, hannanum, kkma, mecab

import os
from datetime import datetime

"""### '괜찮다','맛있다' 라벨 매핑"""

import pandas as pd

df = pd.read_csv('/content/gangnam cafe review.csv', header=0)

# Define a mapping of labels to numerical values
label_mapping = {'맛있다': 0, '괜찮다': 1}

# Replace 'y_pred' with the actual variable containing your predicted labels
y_pred = ['괜찮다', '맛있다']

# Map the labels in the 'score' column to binary values
df['score'] = df['score'].map(label_mapping)

# Convert the string labels in y_pred to integer labels using the mapping
y_pred_int = [label_mapping[label] for label in y_pred]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['score'],test_size =0.2, shuffle = 0.3
                                                    ,stratify=df['score'])
X_train.shape, X_test.shape, y_train.shape, y_test.shape

print('#Train set size:', len(X_train))
print('#Test set size:', len(X_test))

"""###토큰화"""

review = X_train[1]
# review = X_train[0:271]
review

from konlpy.tag import Hannanum, Okt, Kkma

"""###불용어 제거"""

#전처리 함수
def preprocessing(review):
    okt = Okt()

    f = open('/content/gh-stopwords-json-ko.txt')
    stop_words = f.read().split()

    # 1. 한글 및 공백을 제외한 문자 모두 제거.
    review_text = re.sub("[^가-힣\\s]", "", review)

    # 2. okt 객체를 활용해서 형태소 토큰화 + 품사 태깅
    word_review = okt.pos(review_text, stem=True)

    # 노이즈 & 불용어 제거
    word_review = [(token, pos) for token, pos in word_review if not token in stop_words and len(token) > 1]

    # 명사, 동사, 형용사 추출
    word_review = [token for token, pos in word_review if pos in ['Noun', 'Verb', 'Adjective']]

    return word_review

"""###품사태깅

###[분류 모델의 학습 데이터로 변환하기]
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression

bow = CountVectorizer(tokenizer=preprocessing, min_df=5, max_df=0.5, max_features = 2000)

# X_train_bow = bow.fit(X_train)
X_train_bow = bow.fit_transform(X_train)
X_test_bow = bow.transform(X_test)

len(bow.get_feature_names_out())
print(bow.get_feature_names_out())

"""###TF-IDF로 변환"""

tfidf = TfidfVectorizer(tokenizer=preprocessing, max_features=2000, min_df=5, max_df=0.5)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

X_train_tfidf.toarray()

X_train_tfidf.shape

print(tfidf.get_feature_names_out())
print(len(tfidf.get_feature_names_out()))

X_train_tfidf[0].toarray()

X_train_tfidf.shape

"""###<Step3. 분류> : 긍정 부정 리뷰 분류하기"""

# 모델 1 : Logistic Regression 모형
from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression()
log_clf.fit(X_train_bow, y_train)
print('Train set score: {:.3f}'.format(log_clf.score(X_train_bow, y_train)))
print('Test set score: {:.3f}'.format(log_clf.score(X_test_bow, y_test)))

# 모델 2 : 트리 앙상블 모형
from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(n_estimators = 100, max_depth=30, random_state = 0)
model_rf.fit(X_train_tfidf, y_train)
print('Train set score: {:.3f}'.format(model_rf.score(X_train_tfidf, y_train)))
print('Test set score: {:.3f}'.format(model_rf.score(X_test_tfidf, y_test)))

from sklearn.naive_bayes import MultinomialNB

multi_naive = MultinomialNB()
multi_naive.fit(X_train_tfidf, y_train)

print('train score : {}'.format(multi_naive.score(X_train_tfidf, y_train)))
print('test score : {}'.format(multi_naive.score(X_test_tfidf, y_test)))

"""<Step4. 활용> : 중요 키워드 분석"""

# 학습한 회귀 모델의 계수를 출력합니다.
plt.rcParams['figure.figsize'] = [10, 8]
plt.bar(range(len(lr.coef_[0])), lr.coef_[0])

print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)[:5])
print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)[-5:])

"""[중요 피처의 형태소]"""

# 회귀 모델의 계수를 높은 순으로 정렬합니다.
coef_pos_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)

# 회귀 모델의 계수를 높은 순으로 정렬합니다.
coef_pos_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)

# 회귀 모델의 계수를 bow에 맵핑하여, 어떤 형태소인지 출력할 수 있게 합니다.
invert_vectorizer = {v: k for k, v in bow.vocabulary_.items()}

# 계수가 높은 순으로, 피처에 형태소를 맵핑한 결과를 출력합니다. 계수가 높은 피처는 리뷰에 긍정적인 영향을 주는 형태소라고 할 수 있습니다.
print(str(invert_vectorizer)[:100]+'..')

# for coef in coef_pos_index[:20]:
#   print(invert_vectorizer[coef[1]], coef[0])

pos_top_word=[]
pos_top_score=[]

for coef in coef_pos_index:
    print(invert_vectorizer[coef[1]], coef[0])
    pos_top_word.append(invert_vectorizer[coef[1]])
    pos_top_score.append(coef[0])

# Assuming you have already defined and trained your log_clf model
# coef_pos_index should already be defined

# Initialize empty lists for positive words and scores
pos_top_word = []
pos_top_score = []

# Print the top 10 positive keywords and their coefficients
top_10_positive_keywords = coef_pos_index[:10]

print("Top 10 positive keywords:")
for coef, index in top_10_positive_keywords:
    keyword = invert_vectorizer[index]
    score = coef
    pos_top_word.append(keyword)
    pos_top_score.append(score)
    print(f"Keyword: {keyword}, Coefficient: {score}")

# for coef in coef_pos_index[:-20]:
#   print(invert_vectorizer[coef[1]], coef[0])

neg_top_word=[]
neg_top_score=[]

for coef in coef_pos_index:
    print(invert_vectorizer[coef[1]], coef[0])
    pos_top_word.append(invert_vectorizer[coef[1]])
    pos_top_score.append(coef[0])

# Assuming you have already defined and trained your log_clf model
# coef_pos_index should already be defined

# Initialize empty lists for positive and negative words and scores
pos_top_word = []
pos_top_score = []
neg_top_word = []
neg_top_score = []

# Print the top 10 positive keywords and their coefficients
top_10_positive_keywords = coef_pos_index[:11]

print("Top 10 positive keywords:")
for coef, index in top_10_positive_keywords:
    keyword = invert_vectorizer[index]
    score = coef
    pos_top_word.append(keyword)
    pos_top_score.append(score)
    print(f"Keyword: {keyword}, Coefficient: {score}")

# Print the bottom 10 negative keywords and their coefficients
bottom_10_negative_keywords = coef_pos_index[-20:]

print("\nBottom 10 negative keywords:")
for coef, index in bottom_10_negative_keywords:
    keyword = invert_vectorizer[index]
    score = coef
    neg_top_word.append(keyword)
    neg_top_score.append(score)
    print(f"Keyword: {keyword}, Coefficient: {score}")

"""### 긍정어, 부정어 시각화 차트"""

# Assuming you have already defined and trained your log_clf model
# coef_pos_index should already be defined

# Initialize empty lists for positive and negative words and scores
pos_top_word = []
pos_top_score = []
neg_top_word = []
neg_top_score = []

# Extract the top 10 positive keywords and their coefficients
top_10_positive_keywords = coef_pos_index[:10]

for coef, index in top_10_positive_keywords:
    keyword = invert_vectorizer[index]
    score = coef
    pos_top_word.append(keyword)
    pos_top_score.append(score)

# Extract the bottom 10 negative keywords and their coefficients
bottom_10_negative_keywords = coef_pos_index[-10:]

for coef, index in bottom_10_negative_keywords:
    keyword = invert_vectorizer[index]
    score = coef
    neg_top_word.append(keyword)
    neg_top_score.append(score)

import matplotlib.pyplot as plt

# Create a horizontal bar chart for the top 10 positive keywords
plt.figure(figsize=[10, 6])
plt.barh(pos_top_word, pos_top_score, color='g')
plt.legend()
plt.xlabel('키워드별 Vectorized Score')
plt.ylabel('Top 긍정 키워드')
plt.title('상위 10개 긍정 키워드 시각화')
plt.show()

# Create a horizontal bar chart for the bottom 10 negative keywords
plt.figure(figsize=[10, 6])
plt.barh(neg_top_word, neg_top_score, color='r')
plt.legend()
plt.xlabel('키워드별 Vectorized Score')
plt.ylabel('Top 부정 키워드')
plt.title('상위 10개 부정 키워드 시각화')
plt.show()

# Initialize empty lists for top and bottom keywords and their scores
top_keywords = []
top_scores = []
bottom_keywords = []
bottom_scores = []

# Sort the coefficients in descending order
sorted_coef_index = sorted(((value, index) for index, value in enumerate(log_clf.coef_[0])), reverse=True)

# Extract the top 10 and bottom 10 keywords and their scores
top_10 = sorted_coef_index[:10]
bottom_10 = sorted_coef_index[-10:]